// SPDX-FileCopyrightText: 2024 LiveKit, Inc.
//
// SPDX-License-Identifier: Apache-2.0
import {
  type JobContext,
  WorkerOptions,
  cli,
  defineAgent,
  multimodal,
} from "@livekit/agents";
import * as openai from "@livekit/agents-plugin-openai";
import type {
  LocalParticipant,
  Participant,
  TrackPublication,
} from "@livekit/rtc-node";
import { RemoteParticipant, TrackSource } from "@livekit/rtc-node";
import { fileURLToPath } from "node:url";
import { v4 as uuidv4 } from "uuid";

function safeLogConfig(config: SessionConfig): string {
  const safeConfig = { ...config, openaiApiKey: "[REDACTED]" };
  return JSON.stringify(safeConfig);
}

export default defineAgent({
  entry: async (ctx: JobContext) => {
    await ctx.connect();

    const participant = await ctx.waitForParticipant();

    await runMultimodalAgent(ctx, participant);
  },
});

type TurnDetectionType = {
  type: "server_vad";
  threshold?: number;
  prefix_padding_ms?: number;
  silence_duration_ms?: number;
};

interface SessionConfig {
  openaiApiKey: string;
  instructions: string;
  voice: string;
  temperature: number;
  maxOutputTokens?: number;
  modalities: string[];
  turnDetection: TurnDetectionType;
}

function parseSessionConfig(data: any): SessionConfig {
  return {
    openaiApiKey: data.openai_api_key || "",
    instructions: data.instructions || "",
    voice: data.voice || "",
    temperature: parseFloat(data.temperature || "0.8"),
    maxOutputTokens:
      data.max_output_tokens === "inf"
        ? Infinity
        : parseInt(data.max_output_tokens) || undefined,
    modalities: modalitiesFromString(data.modalities || "text_and_audio"),
    turnDetection: data.turn_detection ? JSON.parse(data.turn_detection) : null,
  };
}

function modalitiesFromString(
  modalities: string,
): ["text", "audio"] | ["text"] {
  const modalitiesMap: { [key: string]: ["text", "audio"] | ["text"] } = {
    text_and_audio: ["text", "audio"],
    text_only: ["text"],
  };
  return modalitiesMap[modalities] || ["text", "audio"];
}

function getMicrophoneTrackSid(participant: Participant): string | undefined {
  return Array.from(participant.trackPublications.values()).find(
    (track: TrackPublication) => track.source === TrackSource.SOURCE_MICROPHONE,
  )?.sid;
}

async function runMultimodalAgent(
  ctx: JobContext,
  participant: RemoteParticipant,
) {
  const metadata = JSON.parse(participant.metadata);
  const config = parseSessionConfig(metadata);
  console.log(
    `starting multimodal agent with config: ${safeLogConfig(config)}`,
  );

  const model = new openai.realtime.RealtimeModel({
    apiKey: config.openaiApiKey,
    instructions: config.instructions,
    voice: config.voice,
    temperature: config.temperature,
    maxResponseOutputTokens: config.maxOutputTokens,
    modalities: config.modalities as ["text", "audio"] | ["text"],
    turnDetection: config.turnDetection,
  });

  const agent = new multimodal.MultimodalAgent({ model });
  const session = (await agent.start(
    ctx.room,
  )) as openai.realtime.RealtimeSession;

  session.conversation.item.create({
    type: "message",
    role: "user",
    content: [
      {
        type: "input_text",
        text: "Please begin the interaction with the user in a manner consistent with your instructions.",
      },
    ],
  });
  session.response.create();

  ctx.room.on(
    "participantAttributesChanged",
    (
      changedAttributes: Record<string, string>,
      changedParticipant: Participant,
    ) => {
      if (changedParticipant !== participant) {
        return;
      }
      const newConfig = parseSessionConfig({
        ...changedParticipant.attributes,
        ...changedAttributes,
      });

      session.sessionUpdate({
        instructions: newConfig.instructions,
        temperature: newConfig.temperature,
        maxResponseOutputTokens: newConfig.maxOutputTokens,
        modalities: newConfig.modalities as ["text", "audio"] | ["text"],
        turnDetection: newConfig.turnDetection,
      });
    },
  );

  async function sendTranscription(
    ctx: JobContext,
    participant: Participant,
    trackSid: string,
    segmentId: string,
    text: string,
    isFinal: boolean = true,
  ) {
    const transcription = {
      participantIdentity: participant.identity,
      trackSid: trackSid,
      segments: [
        {
          id: segmentId,
          text: text,
          startTime: BigInt(0),
          endTime: BigInt(0),
          language: "",
          final: isFinal,
        },
      ],
    };
    await (ctx.room.localParticipant as LocalParticipant).publishTranscription(
      transcription,
    );
  }

  session.on("response_done", (response: openai.realtime.RealtimeResponse) => {
    let message: string | undefined;
    if (response.status === "incomplete") {
      if (response.statusDetails?.reason) {
        const reason = response.statusDetails.reason;
        switch (reason) {
          case "max_output_tokens":
            message = "ğŸš« Max output tokens reached";
            break;
          case "content_filter":
            message = "ğŸš« Content filter applied";
            break;
          default:
            message = `ğŸš« Response incomplete: ${reason}`;
            break;
        }
      } else {
        message = "ğŸš« Response incomplete";
      }
    } else if (response.status === "failed") {
      if (response.statusDetails?.error) {
        switch (response.statusDetails.error.code) {
          case "server_error":
            message = `âš ï¸ Server error`;
            break;
          case "rate_limit_error":
            message = `âš ï¸ Rate limit exceeded`;
            break;
          default:
            message = `âš ï¸ Response failed`;
            break;
        }
      } else {
        message = "âš ï¸ Response failed";
      }
    } else {
      return;
    }

    const localParticipant = ctx.room.localParticipant as LocalParticipant;
    const trackSid = getMicrophoneTrackSid(localParticipant);

    if (trackSid) {
      sendTranscription(
        ctx,
        localParticipant,
        trackSid,
        "status-" + uuidv4(),
        message,
      );
    }
  });
}

cli.runApp(new WorkerOptions({ agent: fileURLToPath(import.meta.url) }));
